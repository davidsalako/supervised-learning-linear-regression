# -*- coding: utf-8 -*-
"""Cars4U Project - Supervised Learning Linear Regression Analysis Of A Pan-African Car Sales Technology Organization - By David Salako.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13RVNg_TDwJGYcJg9vsS0AhtU6YCStv9r

# **Cars4U Project - Supervised Learning Linear Regression Analysis Of A Pan-African Car Sales Technology Organization - By David Salako.**

# BACKGROUND & CONTEXT

It is the year 2041. In the imagined near-future there is a supra-national state known as the East and Southern African Union (E.S.A.U.); a powerful political and economic union comprising of the former nation states of Kenya, Uganda, Rwanda, Burundi, Tanzania, Malawi, Mozambuque, South Africa, Lesotho, eSwatini, Zambia, Zimbabwe, Namibia, Botswana, and Angola. 

There is a huge demand for used cars in the E.S.A.U. Market. 

As sales of new cars have slowed down in the recent past, the pre-owned car market has continued to grow over the past years and is larger than the new car market now. Cars4U is a technology organization that aims to find footholes in this market.

In 2038-39, while new car sales were recorded at 3.6 million units, around 4 million second-hand cars were bought and sold. There is a slowdown in new car sales and that could mean that the demand is shifting towards the pre-owned market. In fact, some car sellers replace their old cars with pre-owned cars instead of buying new ones. Unlike new cars, where price and supply are fairly deterministic and managed by OEMs (Original Equipment Manufacturer / except for dealership level discounts which come into play only in the last stage of the customer journey), used cars are very different beasts with huge uncertainty in both pricing and supply. Keeping this in mind, the pricing scheme of these used cars becomes important in order to grow in the market.

As a senior data scientist at Cars4U, you have to come up with a pricing model that can effectively predict the price of used cars and can help the business in devising profitable strategies using differential pricing. For example, if the business knows the market price, it will never sell anything below it. 

# OBJECTIVE

* Explore and visualize the dataset.

* Build a linear regression model to predict the prices of used cars.

* Generate a set of insights and recommendations that will help the business.

<br/>

# DATA DICTIONARY

S.No. : Serial Number

Name : Name of the car which includes Brand name and Model name

Location : The location in which the car is being sold or is available for purchase Cities

Year : Manufacturing year of the car

Kilometers_driven : The total kilometers driven in the car by the previous owner(s) in KM.

Fuel_Type : The type of fuel used by the car. (Petrol, Diesel, Electric, CNG, LPG)

Transmission : The type of transmission used by the car. (Automatic / Manual)

Owner : Type of ownership

Mileage : The standard mileage offered by the car company in kmpl or km/kg

Engine : The displacement volume of the engine in CC.

Power : The maximum power of the engine in bhp.

Seats : The number of seats in the car.

New_Price : The price of a new car of the same model in Rv Rivers. (1 River (Rv) = 100,000 Streams (St))

Price : The price of the used car in Rv Rivers (1 River (Rv) = 100,000 Streams (St))

<br/>

### **East and Southern African Union (E.S.A.U.) Currency**

The currency denominations of the E.S.A.U. are known as the Lake, River, and Stream respectively.
<br/>

100 Rivers (Rv) = 1 Lake (Lk)
<br/>
1 River (Rv) = 100,000 Streams (St)
<br/>
100 Rivers (Rv) = 10,000,000 Streams (St)
<br/>
1 Lake (Lk) = 10,000,000 Streams (St)
<br/>

# Loading libraries
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed rows.
pd.set_option("display.max_columns", None)
# Changes the limit of number of displayed columns tov200
pd.set_option("display.max_rows", 200)

# To build linear model for prediction
from sklearn.linear_model import LinearRegression

# To check model performance
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

"""# Loading and exploring the data

Loading the data into python to explore and understand it.
"""

df = pd.read_csv("esau_used_cars_data.csv")
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")  # f-string

np.random.seed(1)  # To get the same random results every time
df.sample(n=10)

"""OBSERVATIONS ABOVE:

* `S.No.` is just an index for the data entry. In all likelihood, this column will not be a significant factor in determining the price of the car. 
Having said that, there are instances where the index of the data entry contains the information about time factor (an entry with a smaller index corresponds to data entered years ago). Therefore, we will not drop this variable just yet. Let us see if there is any relationship with the price when we do bivariate analysis.

* `Car names` contain a lot of model information. Let us check how many individual names we have. If they are too many, we can process this column to extract important information.

* `Mileage`, `Engine` and `Power` will also need some processing before we are able to explore them. We'll have to extract numerical information from these columns.

* `New Price` column also needs some processing. This one also contains strings and a lot of missing values.
"""

df.info()

"""OBSERVATIONS ABOVE:

As expected, `Mileage`, `Engine`, `Power` and `New_Price` are objects when they should ideally be numerical. To be able to get summary statistics for these columns, we will have to process them first by changing their data types.

# Processing Columns

Let us process 'Mileage', 'Engine', 'Power' and 'New_Price' and extract numerical values from them.

#### **1. Mileage**

We have car mileage in two units, kmpl and km/kg.

After a quick research on the internet it is clear that these 2 units are used for cars of 2 different fuel types.

kmpl - kilometers per litre - is used for petrol and diesel cars.
km/kg - kilometers per kg - is used for CNG and LPG based engines.

We have the variable `Fuel_type` in our data. Let us check if this observations holds true in our data also.
"""

# Create 2 new columns after splitting the mileage values.
km_per_unit_fuel = []
mileage_unit = []

for observation in df["Mileage"]:
    if isinstance(observation, str):
        if (
            observation.split(" ")[0]
            .replace(".", "", 1)
            .isdigit()  # first element should be numeric
            and " " in observation  # space between numeric and unit
            and (
                observation.split(" ")[1]
                == "kmpl"  # units are limited to "kmpl" and "km/kg"
                or observation.split(" ")[1] == "km/kg"
            )
        ):
            km_per_unit_fuel.append(float(observation.split(" ")[0]))
            mileage_unit.append(observation.split(" ")[1])
        else:
            # To detect if there are any observations in the column that do not follow
            # the expected format [number + ' ' + 'kmpl' or 'km/kg']
            print(
                "The data needs further processing. All values are not similar ",
                observation,
            )
    else:
        # If there are any missing values in the mileage column,
        # we add corresponding missing values to the 2 new columns
        km_per_unit_fuel.append(np.nan)
        mileage_unit.append(np.nan)

# No print output from the function above. The values are all in the expected format or NaNs
# Add the new columns to the data

df["km_per_unit_fuel"] = km_per_unit_fuel
df["mileage_unit"] = mileage_unit

# Checking the new dataframe
df.head(5)  # looks good!

# Let us check if the units correspond to the fuel types as expected.
df.groupby(by=["Fuel_Type", "mileage_unit"]).size()

"""OBSERVATIONS ABOVE:

As expected, km/kg is for CNG/LPG cars and kmpl is for Petrol and Diesel cars.

#### **2. Engine**

The data dictionary suggests that `Engine` indicates the displacement volume of the engine in CC.
We will make sure that all the observations follow the same format - [numeric + " " + "CC"] and create a new numeric column from this column. 

This time, lets use a regrex to make all the neccesary checks.
"""

# re module provides support for regular expressions
import re

# Create a new column after splitting the engine values.
engine_num = []

# Regex for numeric + " " + "CC"  format
regex_engine = "^\d+(\.\d+)? CC$"

for observation in df["Engine"]:
    if isinstance(observation, str):
        if re.match(regex_engine, observation):
            engine_num.append(float(observation.split(" ")[0]))
        else:
            # To detect if there are any observations in the column that do not follow [numeric + " " + "CC"]  format
            print(
                "The data needs furthur processing. All values are not similar ",
                observation,
            )
    else:
        # If there are any missing values in the engine column, we add missing values to the new column
        engine_num.append(np.nan)

# No print output from the function above. The values are all in the same format - [numeric + " " + "CC"] OR NaNs
# Add the new column to the data

df["engine_num"] = engine_num

# Checking the new dataframe
df.head(5)

"""#### **3. Power** 

The data dictionary suggests that `Power` indicates the maximum power of the engine in bhp.
We will make sure that all the observations follow the same format - [numeric + " " + "bhp"] and create a new numeric column from this column, like we did for `Engine`
"""

# Create a new column after splitting the power values.
power_num = []

# Regex for numeric + " " + "bhp"  format
regex_power = "^\d+(\.\d+)? bhp$"

for observation in df["Power"]:
    if isinstance(observation, str):
        if re.match(regex_power, observation):
            power_num.append(float(observation.split(" ")[0]))
        else:
            # To detect if there are any observations in the column that do not follow [numeric + " " + "bhp"]  format
            # that we see in the sample output
            print(
                "The data needs furthur processing. All values are not similar ",
                observation,
            )
    else:
        # If there are any missing values in the power column, we add missing values to the new column
        power_num.append(np.nan)

"""OBSERVATIONS ABOVE:

We can see that some Null values in power column exist as 'null bhp' string.
Let us replace these with NaNs
"""

power_num = []

for observation in df["Power"]:
    if isinstance(observation, str):
        if re.match(regex_power, observation):
            power_num.append(float(observation.split(" ")[0]))
        else:
            power_num.append(np.nan)
    else:
        # If there are any missing values in the power column, we add missing values to the new column
        power_num.append(np.nan)

# Add the new column to the data
df["power_num"] = power_num

# Checking the new dataframe
df.head(20)  # looks good now

"""#### **4. New_Price**

We know that `New_Price` is the price of a new car of the same model in Rv Rivers. (1 River = 100,000 Streams (St))

This column clearly has a lot of missing values. We will impute the missing values later. For now we will only extract the numeric values from this column.
"""

# Create a new column after splitting the New_Price values.
new_price_num = []

# Regex for numeric + " " + "River"  format
regex_power = "^\d+(\.\d+)? River$"

for observation in df["New_Price"]:
    if isinstance(observation, str):
        if re.match(regex_power, observation):
            new_price_num.append(float(observation.split(" ")[0]))
        else:
            # To detect if there are any observations in the column that do not follow [numeric + " " + "River"]  format
            # that we see in the sample output
            print(
                "The data needs furthur processing. All values are not similar ",
                observation,
            )
    else:
        # If there are any missing values in the New_Price column, we add missing values to the new column
        new_price_num.append(np.nan)

"""OBSERVATIONS ABOVE:

* Not all values are in Rivers. There are a few observations that are in Lakes (Lk) as well.

* Let us convert these to Rivers. 1 Lake (Lk) = 100 Rivers (Rv)
"""

new_price_num = []

for observation in df["New_Price"]:
    if isinstance(observation, str):
        if re.match(regex_power, observation):
            new_price_num.append(float(observation.split(" ")[0]))
        else:
            # Converting values in Lakes to Rivers
            new_price_num.append(float(observation.split(" ")[0]) * 100)
    else:
        # If there are any missing values in the New_Price column, we add missing values to the new column
        new_price_num.append(np.nan)

# Add the new column to the data
df["new_price_num"] = new_price_num

# Checking the new dataframe
df.head(200)  # Looks ok

"""# Feature Engineering

The `Name` column in the current format might not be very useful in our analysis.
Since the name contains both the brand name and the model name of the vehicle, the column would have to many unique values to be useful in prediction.
"""

df["Name"].nunique()

"""OBSERVATIONS ABOVE:

* With 2041 unique names, car names are not going to be great predictors of the price in our current data.

* But we can process this column to extract important information and see if that reduces the number of levels for this information.

#### **1. Car Brand Name**
"""

# Extract Brand Names
df["Brand"] = df["Name"].apply(lambda x: x.split(" ")[0].lower())

# Check the data
df["Brand"].value_counts()

plt.figure(figsize=(15, 7))
sns.countplot(y="Brand", data=df, order=df["Brand"].value_counts().index)

"""#### **2. Car Model Name**"""

# Extract Model Names
df["Model"] = df["Name"].apply(lambda x: x.split(" ")[1].lower())

# Check the data
df["Model"].value_counts()

plt.figure(figsize=(15, 7))
sns.countplot(y="Model", data=df, order=df["Model"].value_counts().index[1:30])

"""OBSERVATIONS ABOVE:

It is clear from the above charts that out dataset contains used cars from luxury as well as budget friendly brands.

We can create a new variable using this information. We will bin all our cars in 3 categories -

1. Budget Friendly
2. Mid Range
3. Luxury Cars

#### **3. Car Category.**
"""

df.groupby(["Brand"])["Price"].mean().sort_values(ascending=False)

"""OBSERVATIONS ABOVE:

The output is very close to what is expected from domain knowledge, in terms of brand ordering. Mean price of a used Lamborghini is 120 Rivers (1.2 Lakes) and that of cars from other luxury brands follow in a descending order.

Towards the bottom end we have the more budget friendly brands.

We can see that there are missing values in the data set. Let us come back to creating this variable once we have handled the missing data.

# Exploratory Data Analysis
"""

# Basic summary stats - Numeric variables
df.describe().T

"""OBSERVATIONS ABOVE:

1. S.No. clearly has no interpretation here but as discussed earlier let us drop it only after having looked at the initial linear model.
2. Kilometers_Driven values have an incredibly high range. We should check a few of the extreme values to get a sense of the data.
3. Minimum and maximum number of seats in the car also warrent a quick check. On an average a car seems to have 5 seats, which is about right.
4. We have used cars being sold at less than a Stream and as high as 160 Rivers, as we saw for Lamborghini earlier. We might have to drop some of these outliers to build a robust model.
5. Minimum Mileage being 0 is also concerning, we will have to check what is going on.
6. Engine and Power mean and median values are not very different. Only someone with more domain knowledge would be able to comment furthur on these attributes.
7. New price range seems right. We have both budget friendly Maruti cars and Lamborghinis in our stock. Mean being twice that of the median suggests that there are only a few very high range brands, which again makes sense.
"""

# Check Kilometers_Driven extreme values
df.sort_values(by=["Kilometers_Driven"], ascending=False).head(10)

"""OBSERVATIONS ABOVE:

It looks like the first row here is a data entry error. A car manufactured as recently as 2037 (it is currently 2041) having been driven 6,500,000 kms is almost impossible.

The other observations that follow are also on a higher end. There is a good chance that these are outliers. We will look at this furthur while doing the univariate analysis.
"""

# Check Kilometers_Driven Extreme values
df.sort_values(by=["Kilometers_Driven"], ascending=True).head(10)

"""OBSERVATIONS ABOVE:

After looking at the columns - Year, New Price, and Price these entries seem feasible.

1000 might be default value in this case. Quite a few cars having driven exactly 1000 km is suspicious.
"""

# Check seats extreme values
df.sort_values(by=["Seats"], ascending=True).head(5)

"""OBSERVATIONS ABOVE:

Audi A4 having 0 seats is clearly a data entry error. This column warrents some outlier treatment or we can treat seats == 0 as a missing value. Overall, there does not seem not much to be concerned about here.  
"""

# Let us check if we have a similar car in our dataset.
df[df["Name"].str.startswith("Audi A4")]
# Looks like an Audi A4 typically has 5 seats.

# Let us replace #seats in row index 3999 form 0 to 5
df.loc[3999, "Seats"] = 5.0

# Check seats extreme values
df.sort_values(by=["Seats"], ascending=False).head(5)

"""OBSERVATIONS ABOVE:

A Toyota Qualis has 10 seats and so does a Tata Sumo. We do not see any data entry error here.
"""

# Check Mileage - km_per_unit_fuel extreme values
df.sort_values(by=["km_per_unit_fuel"], ascending=True).head(10)

"""OBSERVATIONS ABOVE:

We will have to treat Mileage = 0 as missing values.
"""

# Check Mileage - km_per_unit_fuel extreme values
df.sort_values(by=["km_per_unit_fuel"], ascending=False).head(10)

"""OBSERVATIONS ABOVE:

Maruti Wagon R and Maruti Alto CNG versions are budget friendly cars with high mileage so these data points are fine.
"""

# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later
for colname in df.dtypes[df.dtypes == "object"].index:
    val_counts = df[colname].value_counts(dropna=False)  # Will also show the NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f"Only displaying first {num_to_display} of {len(val_counts)} values.")
    print("\n\n")  # just for more space in between

"""OBSERVATIONS ABOVE:

Since we have not dropped the original columns that we processed, we have a few redunadant output here.

We had checked cars of different `Fuel_Type` earlier, but we did not encounter the 2 electric cars. Let us check why.
"""

df.loc[df["Fuel_Type"] == "Electric"]

"""OBSERVATIONS ABOVE:

Mileage values for these cars are NaN, that is why we did not encounter these earlier with groupby.

Electric cars are very new in the market and very rare in our dataset. We can consider dropping these two observations if they turn out to be outliers later. There is a good chance that we will not be able to create a good price prediction model for electric cars, with the currently available data.

New Price for 6247 entries is missing. We need to explore if we can impute these or we should drop this column altogether.

## Missing Values

Before we start looking at the individual distributions and interactions, let us quickly check for the missing values in the data.
"""

df.isnull().sum()

"""OBSERVATIONS ABOVE:

* 2 Electric car variants don't have entries for Mileage.
* Engine displacement information of 46 observations is missing and maximum power of 175 entries is missing.
* Information about number of seats is not avaliable for 53 entries.
* New Price as we saw earlier has a huge missing count. We'll have to see if there is a pattern here.
* Price is also missing for 1234 entries. Since price is our response variable that we want to predict, we will have to drop these rows when we actually build a model. These rows will not be able to help us in modelling or model evaluation. But while we are analysing the distributions and doing missing value imputations, we will keep using information from these rows.
"""

# Drop the redundant columns.
df.drop(
    columns=["Mileage", "mileage_unit", "Engine", "Power", "New_Price"], inplace=True
)

"""## Distributions

### **Price**
"""

sns.histplot(df["Price"], kde=True)

"""OBSERVATIONS ABOVE:

This is a highly skewed distribution. Let us use log transformation on this column to see of that helps normalise the distribution.
"""

sns.histplot(np.log(df["Price"]), kde=True)
plt.xlabel("Log(Price)")

# Log Transformation has definitely helped in reducing the skew
# Creating a new column with the transformed variable.
df["price_log"] = np.log(df["Price"])
plt.xlabel('Log(Price)')

"""### **Price vs Location**"""

plt.figure(figsize=(15, 7))
sns.boxplot(x="Location", y="Price", data=df)

"""OBSERVATIONS ABOVE:

Price of used cars has a large IQR in Harare and Kigali.

### **Kilometers_Driven**
"""

sns.histplot(df["Kilometers_Driven"], kde=True)

# Log transformation
sns.histplot(np.log(df["Kilometers_Driven"]), kde=True)
plt.xlabel('Log(Kilometers_Driven)')

"""OBSERVATIONS ABOVE:

Transformation has reduced the extreme skewness.
"""

df["kilometers_driven_log"] = np.log(df["Kilometers_Driven"])

"""### Bivariate Distributions"""

sns.pairplot(df, hue="Fuel_Type")

"""OBSERVATIONS ABOVE:

Zooming into these plots gives us a lot of information.

* Contrary to intuition Kilometers Driven does not seem to have a relationship with price.
* Price has a positive relationship with Year. Newer the car, higher the price.
* S.No. does not capture any information that we were hoping for. The temporal element of variation is captured in the year column.
* 2 seater cars are all luxury variants. Cars with 8-10 seats are exclusively mid to high range.
* Mileage does not seem to show much relationship with the price of used cars.
* Engine displacement and Power of the car have a positive relationship with the price.
* New Price and Used Car Price are also positively correlated, which is expected.
* Kilometers Driven has a peculiar relationship with the Year variable. Generally, newer the car lesser the distance it has travelled, but this is not always true.
* CNG cars are conspicuous outliers when it comes to Mileage. The mileage of these cars is very high.
* Mileage and power of newer cars is increasing owing to advancement in technology.
* Mileage has a negative correlation with engine displacement and power. More powerful the engine, more fuel it consumes in general.

### Correlation between numeric Variables
"""

plt.figure(figsize=(12, 7))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")

"""OBSERVATIONS ABOVE:

* Power and engine are important predictors of price
* We will have to work on imputing New Price missing values because this is a very important feature in predicting used car price accurately

## Missing Value Treatment
"""

# Missing values check once again.
df.isnull().sum()

"""### Seats"""

# Look at a few rows where the number of seats is missing.
df[df["Seats"].isnull()]

# We'll impute these missing values one by one, by taking median number of seats for the particular car,
# using the Brand and Model name
df.groupby(["Brand", "Model"], as_index=False)["Seats"].median()

# Impute missing Seats
df["Seats"] = df.groupby(["Brand", "Model"])["Seats"].transform(
    lambda x: x.fillna(x.median())
)

# Check 'Seats'
df[df["Seats"].isnull()]

# Maruti Estilo can accomodate 5
df["Seats"] = df["Seats"].fillna(5.0)

# We will use similar methods to fill missing values for engine, power and new price
df["engine_num"] = df.groupby(["Brand", "Model"])["engine_num"].transform(
    lambda x: x.fillna(x.median())
)
df["power_num"] = df.groupby(["Brand", "Model"])["power_num"].transform(
    lambda x: x.fillna(x.median())
)
df["new_price_num"] = df.groupby(["Brand", "Model"])["new_price_num"].transform(
    lambda x: x.fillna(x.median())
)

df.isnull().sum()

# There are still some missing values in power, mileage and new_price_num.
# There are a few car brands and models in our dataset that do not contain the new price information at all.
# we will estimate the new price using median of the data.

cols1 = ["power_num","km_per_unit_fuel","new_price_num"]

for ii in cols1:
    df[ii] = df[ii].fillna(df[ii].median())

df.isnull().sum()

# Drop the redundant columns.
df.drop(columns=["Kilometers_Driven", "Name", "S.No."], inplace=True)

# Drop the rows where 'Price' == NaN and proceed to modelling
df = df[df["Price"].notna()]

"""# Linear Model Building

1. What we want to predict is the "Price". We will use the normalized version 'price_log' for modelling.
2. Before we proceed to modelling, we will have to encode categorical features. We will drop categorical features like - Name.
3. We will split the data into train and test, to be able to evaluate the model that we build on the train data.
4. Build a Linear Regression model using the train data.
5. Evaluate the model performance on test data.

### Define dependent variable
"""

ind_vars = df.drop(["Price", "price_log"], axis=1)
dep_var = df[["price_log"]]   #The logarithm of prices will be the target variable.

"""### Creating dummy variables"""

def encode_cat_vars(x):
    x = pd.get_dummies(
        x,
        columns=x.select_dtypes(include=["object", "category"]).columns.tolist(),
        drop_first=True,
    )
    return x

ind_vars_num = encode_cat_vars(ind_vars)
ind_vars_num.head()

"""### Split the data into train and test"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    ind_vars_num, dep_var, test_size=0.3, random_state=1
)

print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in train data =", x_test.shape[0])

"""### Fitting a linear model"""

lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train,y_train)

# let us check the coefficients and intercept of the model

coef_df = pd.DataFrame(np.append(lin_reg_model.coef_.flatten(), lin_reg_model.intercept_), \
                       index=x_train.columns.tolist()+['Intercept'], columns=['Coefficients'])
coef_df

"""**Let us check the model performance on training data.**"""

# MAPE
def mape(targets, predictions):
    return np.mean(np.abs((targets - predictions)) / targets) * 100

# Adjusted R^2
def adj_r2(ind_vars, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = ind_vars.shape[0]
    k = ind_vars.shape[1]
    return 1-((1-r2)*(n-1)/(n-k-1))

# Model performance check
def model_perf(model, inp, out):

    y_pred = np.exp(model.predict(inp))
    y_act = np.exp(out.values)

    return pd.DataFrame({
                "RMSE": np.sqrt(mean_squared_error(y_act, y_pred)),
                "MAE": mean_absolute_error(y_act, y_pred),
                "MAPE": mape(y_act, y_pred),
                "R^2": r2_score(y_act, y_pred),
                "Adjusted R^2": adj_r2(inp, y_act, y_pred)
           }, index=[0])

# Checking model performance on train set
print('Training Performance\n')
print(model_perf(lin_reg_model, x_train, y_train))

"""* Both the R-squared and Adjusted R squared of our model are very high. This is a clear indication that we have been able to create a very good model that is able to explain variance in price of used cars for up to 94%.
* The model is not an underfitting model.
* Let us do a quick performance check on the test data.
"""

# Checking model performance on test set
print('Test Performance\n')
print(model_perf(lin_reg_model, x_test, y_test))

"""* RMSE of train and test data is starkly different, indicating that our model is overfitting the train data. 
* Mean Absolute Error indicates that our current model is able to predict used cars prices within mean error of 1.4 Rivers on test data.
* The units of both RMSE and MAE are same - Rivers in this case. But RMSE is greater than MAE because it peanalises the outliers more.
* Mean Absolute Percentage Error is ~13% on the test data.

**Our current model is extremely complex. Let us first bin the Brand and Model columns as it will help us make the dataset more managable.**
"""

df.groupby(["Brand", "Model"])["new_price_num"].mean().sort_values(ascending=False)

"""**We will create a new variable Car Category by binning the new_price_num**"""

# Create a new variable - Car Category
df1 = df.copy()
df1["car_category"] = pd.cut(
    x=df["new_price_num"],
    bins=[0, 15, 30, 50, 200],
    labels=["Budget_Friendly", "Mid-Range", "Luxury_Cars", "Ultra_luxury"],
)
df1["car_category"].value_counts()

# Drop the Brand and Model and new_price_num columns.
df1.drop(columns=["Brand", "Model", "new_price_num"], axis=1, inplace=True)

# We will have to create the x and y datasets again
ind_vars = df1.drop(["Price", "price_log"], axis=1)
dep_var = df1[["price_log"]]

# Dummy encoding
ind_vars_num = encode_cat_vars(ind_vars)

# Splitting data into train and test
x_train2, x_test2, y_train, y_test = train_test_split(
    ind_vars_num, dep_var, test_size=0.3, random_state=1
)

print("Number of rows in train data =", x_train2.shape[0])
print("Number of rows in train data =", x_test2.shape[0])

# fitting linear regression model
lin_reg_model2 = LinearRegression()
lin_reg_model2.fit(x_train2,y_train)

# let us check the coefficients and intercept of the model

coef_df = pd.DataFrame(np.append(lin_reg_model2.coef_.flatten(), lin_reg_model2.intercept_), \
                       index=x_train2.columns.tolist()+['Intercept'], columns=['Coefficients'])
coef_df

# Checking model performance on train set
print('Training Performance\n')
print(model_perf(lin_reg_model2, x_train2, y_train))

# Checking model performance on test set
print('\n\nTest Performance\n')
print(model_perf(lin_reg_model2, x_test2, y_test))

"""- Binning the car brands and models has drastically reduced the model performance.
- Let us try using forward feature selection to see if we can improve the model performance.

### Forward Feature Selection
"""

import six
import sys
sys.modules['sklearn.externals.six'] = six
import joblib
sys.modules['sklearn.externals.joblib'] = joblib

from mlxtend.feature_selection import SequentialFeatureSelector as sfs

reg = LinearRegression()

# Build step forward feature selection
sfs1 = sfs(reg,k_features = x_train2.shape[1], forward=True,   # k_features denotes "Number of features to select"
           floating=False, scoring= 'r2',
           verbose=2, cv=5)

# Perform SFFS
sfs1 = sfs1.fit(x_train2, y_train)

"""**We can see that Adjusted-R square starts decreasing after addition of 21st feature, so we will proceed only with best 20 features**

* Now we will change k_features to 20.
"""

reg = LinearRegression()

# # Build step forward feature selection
sfs1 = sfs(reg, k_features = 20, forward=True,
           floating=False, scoring='r2',
           verbose=2, cv=5)

# Perform SFFS
sfs1 = sfs1.fit(x_train2, y_train)

# Now Which features are important?
feat_cols = list(sfs1.k_feature_idx_)
print(feat_cols)

x_train2.columns[feat_cols]

"""**Now we will fit a sklearn model using these features only.**"""

x_train3 = x_train2[x_train2.columns[feat_cols]]

x_test2.columns

#Creating new x_test with the same 20 variables that we selected for x_train
x_test3 = x_test2[x_train3.columns]

#Fitting linear model
lin_reg_model3 = LinearRegression()
lin_reg_model3.fit(x_train3,y_train)

# let us check the coefficients and intercept of the model

coef_df = pd.DataFrame(np.append(lin_reg_model3.coef_.flatten(), lin_reg_model3.intercept_.flatten()), \
                       index=x_train3.columns.tolist()+['Intercept'], columns=['Coefficients'])
print(coef_df)

# model performance on train set
print('\n\nTraining Performance\n')
print(model_perf(lin_reg_model3, x_train3, y_train))

# model performance on test set
print('\n\nTest Performance\n')
print(model_perf(lin_reg_model3, x_test3, y_test))

"""- Even with forward feature selection we did not achieve good performance.
- We will now try forward feature selection with the data before binning the car brands and models.
"""

from mlxtend.feature_selection import SequentialFeatureSelector as sfs

reg = LinearRegression()

n_features = list(range(5,55,5))

for item in n_features:
    # Build step forward feature selection
    sfs1 = sfs(reg, k_features = item, forward=True,   # k_features denotes "Number of features to select"
               floating=False, scoring= 'r2',
               verbose=0, n_jobs=-2, cv=5)

    # Perform SFFS
    sfs1 = sfs1.fit(x_train, y_train)
    print('R^2:', sfs1.k_score_, '\nFeatures used:', item, sfs1.k_feature_names_, '\n')

"""**We see that after 35 best features, adding more features does not result in a significant increase in performance. So we will move forward with 35 best features, and make a linear regression model using only these features.**"""

# Build step forward feature selection
sfs1 = sfs(reg, k_features = 35, forward=True,   # k_features denotes "Number of features to select"
           floating=False, scoring= 'r2',
           verbose=0, n_jobs=-2, cv=5)

# Perform SFFS
sfs1 = sfs1.fit(x_train, y_train)
feat_cols = list(sfs1.k_feature_names_)

# take the best columns as per forward feature selection
X_train_final = x_train[feat_cols]
X_test_final = x_test[feat_cols]

print(X_train_final.shape, X_test_final.shape)
X_train_final.head()

#Fitting linear model
lin_reg_model4 = LinearRegression()
lin_reg_model4.fit(X_train_final,y_train)

# let us check the coefficients and intercept of the model

coef_df = pd.DataFrame(np.append(lin_reg_model4.coef_.flatten(), lin_reg_model4.intercept_.flatten()), \
                       index=X_train_final.columns.tolist()+['Intercept'], columns=['Coefficients'])
print(coef_df)

# model performance on train set
print('\n\nTraining Performance\n')
print(model_perf(lin_reg_model4, X_train_final, y_train))

# model performance on test set
print('\n\nTest Performance\n')
print(model_perf(lin_reg_model4, X_test_final, y_test))

"""# Observations and Conclusions

1. With our linear regression model we have been able to capture ~85% variation in the data set.

2. The model indicates that the most significant predictors of price of used cars are:

    - The year of manufacturing
    - Price of new car
    - Power of the engine
    - Mileage
    - Kilometers Driven
    - Location
    - Fuel_Type
    - Transmission - Automatic/Manual
    - Car Brand
    - Car Model


3. Newer cars sell for higher prices. One unit increase in the year of manufacture leads to exp(0.1198) = 1.13 Rivers increase in the price of the vehicle, when everything else is constant.

**It is important to note here that the predicted values are log(price) and therefore coefficients have to converted accordingly to understand that influence in Price.**

4. As the price of a new car of same model increases by one unit, the price of the used car increases by exp(0.0029) = 1.00 Rivers.

5. Mileage is inversely correlated with Price. Generally, high Mileage cars are the lower budget cars.

**It is important to note here that correlation is not equal to causation. That is to say that increase in Mileage does not lead to a drop in prices. It can be understood in such a way that the cars with high mileage do not have a high power engine and therefore have low prices.**

6. Kilometers Driven have a negative relationship with the price which is intuitive. A car that has been driven more will have more wear and tear and hence sell at a lower price.

7. The categorical variables are a little hard to interpret. But it can be seen that most of the Brand and Model variables in the dataset have a positive relationship with the Price and the magnitude of this positive relationship increases as the brand category moves to the luxury brands.

# Business Insights and Recommendations

* Our final Linear Regression model has a MAPE of ~18% on The test data, which means that we are able to predict within 18% of the price value. This is a very good model and we can use this model in production. 
* Some markets tend to have higher prices. It might be a good strategy to plan growth in specific cities using this information. Markets like Lusaka (coeff ~ -0.18) are very risky and we need to be careful about investments in this area.
* We will have to analyse the cost side of things before we can talk about profitability in the business. We should gather data regarding that.
* The next step post that would be to cluster different sets of data and see if we should make multiple models for different locations/car types.

### Analysing predictions where we were way off the mark
"""

# Extracting the rows from original data frame df where indexes are same as the training data
original_df = df[df.index.isin(X_train_final.index.values)].copy()

# Extracting predicted values and residuals from the final model
fitted_values = lin_reg_model4.predict(X_train_final)
residuals = fitted_values - y_train

# Add new columns for predicted values
original_df["Predicted price_log "] = fitted_values
original_df["Predicted Price"] = np.exp(fitted_values)
original_df["residuals"] = residuals
original_df["Abs_residuals"] = np.exp(residuals)
original_df["Difference in Rivers"] = np.abs(
    original_df["Price"] - original_df["Predicted Price"]
)

# Let us look at the top 50 predictions where our model made highest extimation errors (on train data)
original_df.sort_values(by=["Difference in Rivers"], ascending=False).head(50)

"""OBSERVATIONS ABOVE:

* A 2017 Land Rover, whose new model sells at 230 Rivers and the used version sold at 160 Rivers was predicted to be sold at < 3 Rivers. It is not apparent after looking at numerical predictors, why our model predicted such low value here. This could be because many other land rovers in our data seems to have sold at lower prices.
* Another entry in the list here is a Lamborghini Gallardo that was sold at 120 Rivers but our model predicted the price as 3.07 Rivers. This is a huge error by the model. However, there might be a data entry error here as the price of a new Gallardo is set at 11.685 Rivers, which is less than the selling price of a used Gallardo.
* There are a few instances where the model predicts lesser than the actual selling price. These could be a cause for concern. The model predicting lesser than potential selling price is not good for business.

Let us quickly visualise some of these observations. 
"""

sns.scatterplot(
    data = original_df,
    x="Difference in Rivers",
    y = "Price",
    hue=original_df["Fuel_Type"],
)

"""OBSERVATIONS ABOVE:

Our model predicts that resale value of diesel cars is higher compared to petrol cars.
"""